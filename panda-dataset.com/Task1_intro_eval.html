<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>ECCV2020 Challenge | PANDA Dataset</title>
    <link rel="icon" href="https://panda-website-1301093743.cos.ap-hongkong.myqcloud.com/images/logo/label-logo.png" sizes="32x32">
    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">
    <link href="css/font-awesome.min.css" rel="stylesheet">
    <link href="css/fontello.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="css/animsition.min.css">
    <!-- Google Fonts -->
    <!-- <link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700|Merriweather:300,300i,400,400i,700,700i" rel="stylesheet"> -->
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elients and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body class="animsition">
    <div class="header">
        <div class="container">
            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-6">
                    <!-- logo -->
                    <div class="logo">
                        <a href="index.html"><img src="	https://panda-website-1301093743.cos.ap-hongkong.myqcloud.com/images/logo/panda-thu-logo.png" alt=" "></a>
                    </div>
                </div>
                <!-- logo -->
                <div class="col-md-8 col-sm-8 col-xs-12">
                    <div id="navigation">
                        <!-- navigation start-->
                        <ul>
                            <li class="active"><a href="index.html" class="animsition-link">Home</a></li>
                            <li><a href="index.html" class="animsition-link">Challenges</a><ul>
                                <li><a href="Task1_intro_eval.html">Task 1: Object Detection</a></li>
                                <li><a href="Task2_intro_eval.html">Task 2: Multi-Object Tracking</a></li>
                            </ul></li>
                            <!-- <li><a href="UnstructuredCam.html" class="animsition-link">UnstructuredCam</a></li>
                            <li><a href="RUSH_Macroscopy.html" class="animsition-link">RUSH Macroscopy</a></li>
                            <li><a href="http://www.gigavision.cn/" class="animsition-link">GigaVision</a></li> -->
                            <li><a href="Download.html" class="animsition-link">Download</a></li>
                            <!-- <li><a href="#" class="animsition-link">Leaderboards</a><ul>
                                <li><a href="#">Task1: Object Detection</a></li>
                                <li><a href="#">Task2: Multi-Object Tracking</a></li>
                            </ul></li> -->
                            <li><a href="FAQ.html" class="animsition-link">FAQ</a></li>
                            <li><a href="Team.html" class="animsition-link">Team</a></li>
                        </ul>
                    </div>
                    <!-- /.navigation start-->
                </div>
            </div>
        </div>
    </div>

    <div class="page-header">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <div class="page-breadcrumb">
                        <ol class="breadcrumb">
                            <li><a href="index.html">Home</a></li>
                            <li><a href="#">challenges</a></li>
                            <li class="active">Task1</li>
                        </ol>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="section-space20 bg-white">
        <!-- content start -->
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <div class="wrapper-content bg-white">
                        <div class="team-section text-justify pinside40">
                            <div class="row">
                                <h1>Task1: Pedestrian & Vehicle Detection</h1>

                                <p><img src="https://panda-website-1301093743.cos.ap-hongkong.myqcloud.com/images/home/info1.jpg" alt="" align="middle" width="110%" /></p>
                                <h2>Introduction</h2>
                                <p>Deep learning based computer vision algorithms have surpassed the human-level performance for many CV tasks, like object recognition and face verification. Object detection is a fundamental task for human-centric visual analysis. The extremely high resolution of PANDA makes it possible to detect objects from a long distance. However, the significant variance in scale, posture, and occlusion severely degrade the detection performance.</p>
                                
                                <p>This task is designed to push the state-of-the-art in object detection on giga-pixel images forward. Teams are required to predict the bounding boxes of objects of pedestrians and vehicles with real-valued confidences.</p>

                                <p>Challenge participants are required to detect two types of targets, pedestrians and vehicles. For each pedestrian, three bounding-boxes should be submitted: visible body bbox, full body bbox, and head bbox. For each vehicle, a visible part bbox needs to be submitted. Some special regions (e.g., fake persons, extremely crowded regions, heavily occluded persons, etc.) are ignored in evaluation.</p>

                                <p>The challenge is based on PANDA-Image dataset which contains 555 static giga-pixel images (390 for training, 165 for testing) captured by giga-pixel camera in different places at different height. We manually annotate the bounding boxes of different categories of objects in each image. Specifically, each person is annotated by 3 box, visible body box, full body box, and head box. All data and annotations on the training set are publicly available. Please see the <a href="Download.html" class="animsition-link">Download</a> page for more details about annotation. </p>

                               
                                <h2>Results Format</h2>
                               <p>The format of the result file is the same as that of the <a href="http://cocodataset.org/#format-results">COCO Challenge</a>. We require the participator to submit the results as a <em>single <strong>det_results.json</strong> file</em> (save via gason in Matlab or json.dump in Python). This .json file should contain a list whose each element is a dictionary. Each dictionary contains information about a result box, whose format is as follows:</p>
                                <p>
                                <code>
                                    [{

                                        "image_id": int,
                                    
                                        "category_id": int,
                                    
                                        "bbox": [bbox_left, bbox_top, bbox_width, bbox_height],
                                    
                                        "score": float
                                    
                                    }]
                                </code>
                                </p>
                               
                                <p>The meaning of each value is listed as follows:</p>
                               
                               <div class="section-space20 pinside20 bg-primary">
                                   <table border="1" width="80%" align="center">
                                       <tbody align="center">
                                           <tr>
                                               <td>Key</td>
                                               <td>Description</td>
                                           </tr>
                                           <tr>
                                            <td>image_id</td>
                                            <td>The serial number of the image, which <strong>shall be consistent with the annotation file</strong></td>
                                           </tr>
                                           <tr>
                                            <td>category_id</td>
                                            <td> the type of detection result box. <strong>shall be consistent with the following table</strong></td>
                                           </tr>
                                           <tr>
                                               <td>bbox_left</td>
                                               <td>The x coordinate of the top-left corner of the predicted bounding box</td>
                                           </tr>
                                           <tr>
                                               <td>bbox_top</td>
                                               <td>The y coordinate of the top-left corner of the predicted object bounding box</td>
                                           </tr>
                                           <tr>
                                               <td>bbox_width</td>
                                               <td>The width in pixels of the predicted object bounding box</td>
                                           </tr>
                                           <tr>
                                               <td>bbox_height</td>
                                               <td>The height in pixels of the predicted object bounding box</td>
                                           </tr>
                                           <tr>
                                               <td>score</td>
                                               <td>The confidence of the predicted bounding box enclosing an object instance</td>
                                           </tr>
                                       </tbody>
                                   </table>

                                   <p></p>
                                   <table border="1" width="80%" align="center">
                                    <tbody align="center">
                                        <tr>
                                            <td>Object</td>
                                            <td>category_id</td>
                                        </tr>
                                        <tr>
                                         <td>person visible body</td>
                                         <td>1</td>
                                        </tr>
                                        <tr>
                                         <td>person full body</td>
                                         <td>2</td>
                                        </tr>
                                        <tr>
                                        <td>person head</td>
                                        <td>3</td>
                                        </tr>
                                        <tr>
                                        <td>vehicle visible part</td>
                                        <td>4</td>
                                        </tr>
                                    </tbody>
                                </table>
                               </div>


                               <h2>Evaluation Metrics</h2>
                               <p>We require each evaluated algorithm to output a list of detected bounding boxes with confidence scores for each test image in the predefined format. Please see the results format above for more detail. Similar to the evaluation protocol in <a href="http://cocodataset.org/#detection-eval">COCO Challenge</a> [1], we use  <code>AP</code>, <code>AP<sub>IOU=0.50</sub></code>, <code>AP<sub>IOU=0.75</sub></code>, <code>AR<sub>max=10</sub></code>, <code>AR<sub>max=100</sub></code>, and <code>AR<sub>max=500</sub></code> metrics to evaluate the results of detection algorithms. Unless otherwise specified, the AP and AR metrics are averaged over multiple intersection over union (IoU) values. Specifically, we use ten IoU thresholds of <code>[0.50:0.05:0.95]</code>. All metrics are computed allowing for at most 500 top-scoring detections per image (across all categories). These criteria penalize missing detection of objects as well as duplicate detections (two detection results for the same object instance). The AP metric is used as the primary metric for ranking the algorithms. The metrics are described in the following table.</p>
                               <p>The above metrics are calculated over object categories of interest. For comprehensive evaluation, we will report the performance of each object category. Some special regions (e.g., fake persons, extremely crowded regions, heavily occluded persons, etc.) are ignored in evaluation. Please also see the <a href="Download.html" class="animsition-link">Download</a> page for more details about annotation. The evaluation code for object detection in images is available on the <a href="https://github.com/GigaVision/PANDA-Toolkit">PANDA-Toolkit</a>. </p>
                               <div class="section-space20 pinside20 bg-primary">
                                   <table border="1" width="80%" align="center">
                                       <tbody align="center">
                                           <tr>
                                               <td>Measure</td>
                                               <td>Perfect</td>
                                               <td>Description</td>
                                           </tr>
                                           <tr>
                                               <td>AP</td>
                                               <td>100%</td>
                                               <td>The average precision over all 10 IoU thresholds (i.e., [0.5:0.05:0.95]) of all object categories</td>
                                           </tr>
                                           <tr>
                                               <td>AP<sub>IOU=0.50</sub></td>
                                               <td>100%</td>
                                               <td>The average precision over all object categories when the IoU overlap with ground truth is larger than 0.50</td>
                                           </tr>
                                           <tr>
                                               <td>AP<sub>IOU=0.75</sub></td>
                                               <td>100%</td>
                                               <td>The average precision over all object categories when the IoU overlap with ground truth is larger than 0.75</td>
                                           </tr>
                                           <!-- <tr>
                                               <td>ARmax=1</td>
                                               <td>100%</td>
                                               <td>The maximum recall given 1 detection per image</td>
                                           </tr> -->
                                           <tr>
                                               <td>AR<sub>max=10</sub></td>
                                               <td>100%</td>
                                               <td>The maximum recall given 10 detections per image</td>
                                           </tr>
                                           <tr>
                                               <td>AR<sub>max=100</sub></td>
                                               <td>100%</td>
                                               <td>The maximum recall given 100 detections per image</td>
                                           </tr>
                                           <tr>
                                               <td>AR<sub>max=500</sub></td>
                                               <td>100%</td>
                                               <td>The maximum recall given 500 detections per image</td>
                                           </tr>
                                       </tbody>
                                   </table>
                               </div>
                               
                               <h2>Baseline Results</h2>
                               <p><img src="https://panda-website-1301093743.cos.ap-hongkong.myqcloud.com/images/challenge/det-s.png" alt="" align="middle" width="100%" /></p>
                                <p>Table: Performance of detection methods on PANDA. FR, CR, and RN denote Faster R-CNN, Cascade R-CNN and RetinaNet respectively. Sub means subset of different target sizes, where Small, Middle, and Large indicate object size being smaller than 32 × 32, 32 × 32 ~ 96 × 96, and large than 96 × 96.</p>
                                
                               <h2>Data and Annotations</h2>
                               <p>For PANDA-Image, all data and annotations for training set are available on the <a href="Download.html" class="animsition-link">Download</a> page.</p>

                               <h2>Tools and Instructions</h2>
                               <p>We provide extensive toolkit support for the PANDA in which APIs for data visualization, split, merge, and result evaluation are provided. Please visit our <a href="https://github.com/GigaVision/PANDA-Toolkit">GitHub repository</a> page. For addition questions, please find the answers in <a href="FAQ.html" class="animsition-link">FAQ</a> or <a href="mailto:zhang-xy18@mails.tsinghua.edu.cn">contact us</a>.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- /.content end -->
    
    <div class="footer section-space20">
        <!-- footer -->
        <div class="container">
            <div class="row">
                <div class="col-md-4 col-sm-4 col-xs-6">
                    <div class="widget-footer">
                        <!-- widget footer -->
                        <ul class="listnone">
                            <li><a href="http://www.gigavision.cn">GigaVision</a></li>
                            <li><a href="https://github.com/GigaVision/PANDA-Toolkit">PANDA Dataset Toolkit</a></li>
                            <li><a href="http://cvpr2020.thecvf.com/">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2020</a></li>
                        </ul>
                    </div>
                    <!-- /.widget footer -->
                </div>
                <div class="col-md-4 col-sm-4 col-xs-6">
                    <div class="widget-footer">
                        <!-- widget footer -->
                        <ul class="listnone">
                            <li><a href="http://www.luvision.net/">LuVision</a></li>
                            <li>Email: <a href="mailto:wanggy21@mails.tsinghua.edu.cn">wanggy21@mails.tsinghua.edu.cn</a></li>
                            <li>Postal addresses: FIT Building, Tsinghua University, Beijing, China.</li>
                        </ul>
                    </div>
                    <!-- /.widget footer -->
                </div>
                <div class="col-md-4 col-sm-4 col-xs-6">
                    <div class="widget-footer">
                        <!-- widget footer -->
                        <ul class="listnone">
                            <li><p>This work is supported in part by NSFC under contract No. 61722209 and 61860206003.</p></li>
                            <li><p>We thank <a href="http://www.aqueti.com/">Aqueti（China）Technology Inc., Co.</a> for providing part of raw datasets and Baidu Inc. for providing part of annotations.</p></li>
                        </ul>
                    </div>
                    <!-- /.widget footer -->
                </div>
            </div>
        </div>
    </div>
    <!-- /.footer -->
    <div class="tiny-footer">
        <!-- tiny footer -->
        <div class="container">
            <div class="row text-center">
                <div class="col-md-12 col-sm-12 col-xs-12">
                    <p>Industry-University-Research Collaboration is welcome via E-mail: fanglu@tsinghua.edu.cn</br>
                    Copyright &copy; luvision.net</p>
                </div>
            </div>
        </div>
    </div>
    
    <!-- /.tiny footer -->
    <!-- back to top icon -->
    <a href="#0" class="cd-top" title="Go to top">Top</a>
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="js/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
    <script type="text/javascript" src="js/menumaker.js"></script>
    <!-- animsition -->
    <script type="text/javascript" src="js/animsition.js"></script>
    <script type="text/javascript" src="js/animsition-script.js"></script>
    <!-- sticky header -->
    <script type="text/javascript" src="js/jquery.sticky.js"></script>
    <script type="text/javascript" src="js/sticky-header.js"></script>
    <!-- slider script -->
    <script type="text/javascript" src="js/owl.carousel.min.js"></script>
    <script type="text/javascript" src="js/slider-carousel.js"></script>
    <script type="text/javascript" src="js/service-carousel.js"></script>
    <!-- Back to top script -->
    <script src="js/back-to-top.js" type="text/javascript"></script>
</body>

</html>

